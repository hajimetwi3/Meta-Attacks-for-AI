## Meta Attacks for AI: Human and AI Reviewers Under Attack  

## Official Documentation

Latest documentation (GitHub Pages):
https://hajimetwi3.github.io/Meta-Attacks-for-AI/

## Preprint  
  
Zenodo (Latest version):  
[https://doi.org/10.5281/zenodo.18248787](https://doi.org/10.5281/zenodo.18248787)  

## Abstract (from the Preprint)  
  
The abstract below is quoted from the preprint version 1.0.
For the most up-to-date version, please refer to the Zenodo record.  

```  
Currently, guardrails are implemented in generative AI systems, such as chat-based AI, 
to detect malicious users and to prevent harmful outputs through automated filtering 
and content moderation in human-in-the-loop AI systems that involve both automated and 
human review.

However, explicit attacks that target the guardrails themselves remain underexplored. 
In this work, I examine this problem and define the following concepts:

Meta Attacks / Infiltration for AI
- Guardrail saturation attacks (Saturation Attacks), including:
  - AI-review-based guardrail saturation attacks
  - Human-review-based guardrail saturation attacks
- Exposure enhancement via AI human review (Exposure Enhancement)
- Attacks on AI human reviewers (i.e., content moderators) via guardrails
- Attacks on AI reviewers via guardrails

Unlike jailbreak or prompt injection attacks that aim to extract prohibited outputs, 
these Meta Attacks target the guardrail mechanisms themselves, including human and AI reviewers. 

```

---  

## Author  
  
First proposed by Hajime Tsui  
X (Twitter): https://x.com/hajimetwi3  
GitHub: https://github.com/hajimetwi3  
Reddit: https://www.reddit.com/user/hajimetwi3/  

